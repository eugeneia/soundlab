\section{Rendering signals}

Before discussing signal synthesis, we must define ways for consuming the
synthesized signal as well as for verification of our results. Because
our domain is music, we need to be able to play back signals as sound.
Furthermore visualizing a signal can be useful for debugging since some
properties of a signal are better conceived visually than aurally.

For both forms of presentation a technique called \textit{sampling} is
used---which will not be described in detail here. All that is needed
to know for this approach, is that the sampling routine records a
sequence of linear amplitude values according to a time span and a
function---or signal---which maps values of time to values of
amplitude. The resulting sequence resembles the kind of data that can be
fed into standard digital sound adapters or plotting applications.

\begin{verbatim}
;;; Approximate type of a sampling function.

(FUNCTION ((FUNCTION (REAL) REAL) REAL)
          (SEQUENCE REAL))
\end{verbatim}

\texttt{SOUNDLAB} derives its signal type from this rationale. It also
exports two functions which record signals to standard \textit{WAVE}
audio files and \textit{Gnuplot} compatible data files
respectively. \texttt{SOUNDLAB} also chooses arbitrary but sensible units
and scales for time and amplitude. Time is chosen to be a number in
seconds greater than zero and amplitude is chosen to be a number ranging
from $-1$ to $1$. Results of inputs to the sampling routine exceeding
these bounds are undefined.
